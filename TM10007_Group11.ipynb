{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christievanderuit/TM10007_group11/blob/main/TM10007_Group11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn numpy matplotlib"
      ],
      "metadata": {
        "id": "cWwNaEqJ_iat",
        "outputId": "2dd79f16-8fe1-4526-853b-47a32f18ec3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2955 sha256=c9b6b26b590bb56e6f3fc876b5b61b3a03879c538c75dd655ae5beccda77963c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/e0/3d/9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General packages\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection \n",
        "from sklearn import preprocessing\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "oS9eMkou8yR_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some functions we will use\n",
        "def colorplot(clf, ax, x, y, h=100):\n",
        "    '''\n",
        "    Overlay the decision areas as colors in an axes.\n",
        "    \n",
        "    Input:\n",
        "        clf: trained classifier\n",
        "        ax: axis to overlay color mesh on\n",
        "        x: feature on x-axis\n",
        "        y: feature on y-axis\n",
        "        h(optional): steps in the mesh\n",
        "    '''\n",
        "    # Create a meshgrid the size of the axis\n",
        "    xstep = (x.max() - x.min() ) / 20.0\n",
        "    ystep = (y.max() - y.min() ) / 20.0\n",
        "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
        "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
        "    h = max((x_max - x_min, y_max - y_min))/h\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    if hasattr(clf, \"decision_function\"):\n",
        "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    else:\n",
        "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    if len(Z.shape) > 1:\n",
        "        Z = Z[:, 1]\n",
        "    \n",
        "    # Put the result into a color plot\n",
        "    cm = plt.cm.RdBu_r\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm"
      ],
      "metadata": {
        "id": "Y8zNyBBH9BCh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "import os\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/tm10007_ml/worcgist/GIST_radiomicFeatures.csv', index_col=0)\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ],
      "metadata": {
        "id": "qX1LjCmltopI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ac1265-04d2-4ada-fe77-df964f75616b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tm10007_ml'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 83 (delta 13), reused 12 (delta 12), pack-reused 61\u001b[K\n",
            "Unpacking objects: 100% (83/83), 67.93 MiB | 8.63 MiB/s, done.\n",
            "The number of samples: 246\n",
            "The number of columns: 494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zorgen dat verschillende functies uit de files aangeroepen kunnen worden \n",
        "from tm10007_ml.worcgist.load_data import *   #misschien dat dit nog op een nettere manier kan \n",
        "dataset = load_data(); "
      ],
      "metadata": {
        "id": "bG7AZdOO5cDq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zoeken van NaN --> want duidelijk dat deze erin zitten - ook voor de nu geplotte in de eerste figuur \n",
        "x_data = data.drop(\"label\", axis='columns')\n",
        "\n",
        "\n",
        "np.where(np.asanyarray(np.isnan(x_data))) \n",
        "\n",
        "# # Find all entries that are not a number\n",
        "x_data = x_data.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
        "nan_b = pd.isna(x_data)\n",
        "n_nan = nan_b.sum().sum()\n",
        "if n_nan > 0:\n",
        "  nan = np.where(nan_b == \"True\")\n",
        "else:\n",
        "  nan = 0\n",
        "# # Replace all NaN with 0\n",
        "# x_train_nieuw = x_train.fillna(0)\n",
        "print(n_nan)\n",
        "\n",
        "# volgens mij hebben wij dus alleen rijen waarbij er opeens 0'en zijn, maar \n",
        "# hebben wij geen Nan \n",
        "#op deze manier worden in x_train alle kolommen gevonden waarbij er alleen maar \n",
        "#0'en zijn, dus deze droppen - dit zijn 6 kolommen \n",
        "df1 = x_data.mask(x_data != 0).dropna(axis=1)\n",
        "# print(df1)\n",
        "\n",
        "#zorgen dat deze rijen verwijderd worden ? willen we dit ?  NOG NIET GELUKT :)\n",
        "# column_names = list(df1.columns.values)\n",
        "# df2 = x_train.drop('index', inplace=True, axis=1)\n",
        "\n",
        "df1 = x_data.mask(x_data != 0).dropna(axis=1)\n",
        "\n",
        "df2 = x_data.copy()\n",
        "for col in df1:\n",
        "    df2 = df2.drop(col, axis=1)\n",
        "\n",
        "#print(df2)"
      ],
      "metadata": {
        "id": "X4w4SMYwvTtm",
        "outputId": "55e9beaf-63f7-4dc3-bf2e-dd98a99437a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitsen in een train en in een testset \n",
        "\n",
        "# Classifiers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Waarbij x = features, y = label\n",
        "#vraag: doet ie dit elke keer op een andere manier? want dan toch lastig conclusies te trekken? --> daarom achter random state een getal - maakt niet uit zolang integer is\n",
        "y = data['label']\n",
        "x = df2\n",
        "# df_train, df_test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1, stratify=y)\n",
        "#AFBLIJVEN VAN TESTDATA!! :) \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "akTrzKpG86zA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testen of er outliers zijn dmv LOF\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "lof = LocalOutlierFactor()\n",
        "x_lof = lof.fit(x_train)\n",
        "outlier_scores = x_lof.negative_outlier_factor_\n",
        "threshold = np.mean(outlier_scores)\n",
        "outliers = x_train[outlier_scores < threshold]\n",
        "outlier_percentage = (len(outliers) / len(x_train)) * 100\n",
        "\n",
        "print(\"Number of outliers:\", len(outliers))\n",
        "print(\"Percentage of outliers:\", outlier_percentage)"
      ],
      "metadata": {
        "id": "sR_vwqT-JiEi",
        "outputId": "8cee600e-ccda-4322-a018-c35c41657967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of outliers: 9\n",
            "Percentage of outliers: 4.591836734693878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testen of de data normaal verdeeld is of niet \n",
        "from scipy.stats import shapiro\n",
        "  \n",
        "# define a function to test for normality using the Shapiro-Wilk test\n",
        "def test_normality(column):\n",
        "    stat, p = shapiro(column)\n",
        "    alpha = 0.05\n",
        "    if p > alpha:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# apply the test_normality function to each column of the DataFrame\n",
        "normality_results = x_train.apply(test_normality)\n",
        "\n",
        "# print the results\n",
        "print(normality_results)\n",
        "# sum = sum(normality_results)\n",
        "print(sum)\n",
        "\n",
        "#dus meeste data is niet normaal verdeeld voor 60 van de gevallen --> dus voor deze wil je \n",
        "#een andere schaling gaan toepassen \n",
        "\n",
        "# losse dataframes maken \n",
        "normalized = pd.DataFrame()\n",
        "non_normalized = pd.DataFrame()\n",
        "for ind, elem in zip(x_train.columns, normality_results): \n",
        "  if elem == 0: \n",
        "    non_normalized = non_normalized.append(x_train[ind])\n",
        "  else: \n",
        "    normalized = normalized.append(x_train[ind])\n",
        "\n",
        "  \n",
        "\n",
        "#los scalen \n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(non_normalized)\n",
        "non_normalized = pd.DataFrame(scaler.transform(non_normalized))\n",
        "\n",
        "scaler_norm = preprocessing.StandardScaler()\n",
        "scaler_norm.fit(normalized)\n",
        "normalized = pd.DataFrame(scaler_norm.transform(normalized))\n",
        "\n",
        "#non_normalized en normalized gecombineerd in een nieuwe dataframe\n",
        "# x_train_scaled = pd.concat([non_normalized, normalized])\n",
        "# scaler_tot = preprocessing.MinMaxScaler()\n",
        "# scaler_tot.fit(x_train_scaled)\n",
        "# x_train_scaled = pd.DataFrame(scaler_tot.transform(x_train_scaled))\n",
        "# x_train_scaled = x_train_scaled.transpose()\n",
        "scaler.fit(x_train)\n",
        "x_scaled = pd.DataFrame(scaler.transform(x_train))\n",
        "\n",
        "#x_train_scaled = normalized.append(non_normalized)\n",
        "# 1. losse df voor normalized en non normalized \n",
        "# 2. scaling met Robust en Standard \n",
        "# 3. Samenvoegen en alles met Min max normalizeren \n"
      ],
      "metadata": {
        "id": "Lmhiy0kOuuh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotten \n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "print(type(x_train['PREDICT_original_hf_min']))\n",
        "plt.scatter(x_train['PREDICT_original_sf_compactness_avg_2.5D'], x_train['PREDICT_original_sf_roughness_std_2.5D'])\n",
        "# plt.show()\n",
        "\n",
        "#Testen met een andere variabele \n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "print(type(x_train['PREDICT_original_hf_min']))\n",
        "plt.scatter(x_train['PREDICT_original_sf_convexity_avg_2.5D'], x_train['PREDICT_original_phasef_phasesym_range_WL3_N5'])\n",
        "# plt.show()\n",
        "\n",
        "print(x_train)"
      ],
      "metadata": {
        "id": "DV4adj_J8qWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt at PCA\n",
        "\n",
        "def pca_func(X,n):\n",
        "  # create PCA object with 2 components\n",
        "  pca = PCA(n_components=n)\n",
        "\n",
        "  # fit and transform the data\n",
        "  df_pca = pca.fit_transform(X)\n",
        "  return df_pca"
      ],
      "metadata": {
        "id": "Ndqro_rJkWD8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.feature_selection import f_classif\n",
        "# def kbest_func(X, k):\n",
        "#     \"\"\"\n",
        "#     Select the top k features using the f_regression score as the evaluation metric\n",
        "#     \"\"\"\n",
        "#     # define the feature selector\n",
        "#     selector = SelectKBest(score_func=f_regression, k=k)\n",
        "    \n",
        "#     # fit the selector to the training data and transform the data\n",
        "#     x_train_kbest = selector.fit_transform(X, y_train)\n",
        "    \n",
        "#     return x_train_kbest\n",
        "\n",
        "def kbest_func(X, k):\n",
        "    \"\"\"\n",
        "    Select the top k features using the f_regression score as the evaluation metric\n",
        "    \"\"\"\n",
        "    # define the feature selector\n",
        "    selector = SelectKBest(f_classif, k=k)\n",
        "    \n",
        "    # fit the selector to the training data and transform the data\n",
        "    x_train_kbest = selector.fit_transform(X, y_train)\n",
        "    return x_train_kbest"
      ],
      "metadata": {
        "id": "1btbKWNlLvc3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE\n",
        "\n",
        "# def rfe_func(X,k):\n",
        "#     # linear regression\n",
        "#     lr = LinearRegression()\n",
        "\n",
        "#     # Create an RFE object and select the top n features (we moeten misschien nog even letten op de step)\n",
        "#     rfe = RFE(estimator=lr, n_features=k, step=1)\n",
        "  \n",
        "#     #fit and transform the data\n",
        "#     df_rfe = rfe.fit_transform(X)\n",
        "#     return df_rfe\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def rfe_func(X,k): \n",
        "    # Create a LabelEncoder object \n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Fit the LabelEncoder object on the target variable (dit wordt gedaan omdat y_train uit alleen maar strings bestaat en dan kan je niet RFE gebruiken)\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "\n",
        "    # Create a logistic regression object\n",
        "    lr = LinearRegression()\n",
        "\n",
        "    # Create an RFE object and fit it on the training data\n",
        "    rfe = RFE(estimator=lr, n_features_to_select=k)\n",
        "    x_train_rfe = rfe.fit_transform(X, y_train_encoded)\n",
        "    return x_train_rfe\n"
      ],
      "metadata": {
        "id": "gIICUveAPhUC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EGAQlKHoTVLI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L1 regularization (Lasso regression)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "def lasso_func(X,k):   \n",
        "    # Create a LabelEncoder object\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Fit the LabelEncoder object on the target variable\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "\n",
        "    # Lasso regression\n",
        "    lasso = Lasso(alpha=k)\n",
        "    \n",
        "    # Create a SelectFromModel object and fit it on the training data\n",
        "    selector = SelectFromModel(estimator=lasso)\n",
        "    selector.fit(X, y_train_encoded)\n",
        "\n",
        "    # Use the selector object to transform the data\n",
        "    x_train_lasso = selector.transform(X)\n",
        "    return x_train_lasso"
      ],
      "metadata": {
        "id": "UFsohvQ1URR2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with cross vallidation\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "n = 50\n",
        "x_pca = pca_func(x_scaled,n)\n",
        "\n",
        "y_gist = pd.Series([1 if label == \"GIST\" else 0 for label in y_train], index=y_train.index)\n",
        "\n",
        "\n",
        "# Create a linear SVM object\n",
        "svm = LinearSVC(max_iter=1000)\n",
        "\n",
        "# Perform 5-fold cross-validation on the training data\n",
        "scores = cross_val_score(svm, x_pca, y_gist, cv=50)\n",
        "\n",
        "# Print the average accuracy and standard deviation of the cross-validation scores\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))\n"
      ],
      "metadata": {
        "id": "G7QNF2o9rY_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging - PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "n = 50\n",
        "x_pca = pca_func(x_scaled,n)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=9)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "scores = cross_val_score(knn, x_pca, y_gist, cv=100)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))\n"
      ],
      "metadata": {
        "id": "jjfT4toUuRbu",
        "outputId": "ef78f98a-bc6e-48ac-b3b3-2155e5f4c60e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 96 members, which is less than n_splits=100.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 48.00% (+/- 36.69%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging - selectKBest\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "k = 100\n",
        "x_kbest = kbest_func(x_scaled,k)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=9)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "scores = cross_val_score(knn, x_kbest, y_gist, cv=100)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "aqx1AjZsTW3d",
        "outputId": "dfd3fca6-b5f4-46a3-be24-a85bac6fa02a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 77  83  84  90  97 103 110] are constant.\n",
            "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 96 members, which is less than n_splits=100.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 61.00% (+/- 36.46%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging - RFE\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "k = 100\n",
        "x_rfe = rfe_func(x_scaled,k)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=9)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "scores = cross_val_score(knn, x_rfe, y_gist, cv=100)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "5VXKQLHwo0Sc",
        "outputId": "9713451e-5b23-44ca-de7a-f94eada981a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 96 members, which is less than n_splits=100.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 59.00% (+/- 33.45%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging - lasso\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "k = 0.099\n",
        "x_lasso = lasso_func(x_scaled, k)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=9)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "scores = cross_val_score(knn, x_lasso, y_gist, cv=100)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "UurZYnzVq89W",
        "outputId": "1542d928-c1d6-4100-ec7c-0c07125773b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 96 members, which is less than n_splits=100.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.50% (+/- 35.56%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LPIhLgIEpKuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non linear SVM - PCA\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "n = 100\n",
        "x_pca = pca_func(x_scaled,n)\n",
        "\n",
        "# Train non-linear SVM with cross-validation\n",
        "clf = SVC(kernel='poly',degree=3, C=1.0)\n",
        "scores = cross_val_score(clf, x_pca, y_train, cv=100)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))\n"
      ],
      "metadata": {
        "id": "KuCb4wFaAyCd",
        "outputId": "b0e068ce-ffd0-4ac1-aca4-789b77e56d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 96 members, which is less than n_splits=100.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 52.00% (+/- 12.08%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Non linear SVM - SelectKBest\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "k = 100\n",
        "x_kbest = kbest_func(x_scaled,k)\n",
        "\n",
        "# Train non-linear SVM with cross-validation\n",
        "clf = SVC(kernel='poly',degree=3, C=1.0)\n",
        "scores = cross_val_score(clf, x_kbest, y_train, cv=100)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "awaeXO3lz7Jt",
        "outputId": "13f894e6-adf0-4287-d61b-265708f88034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 77  83  84  90  97 103 110] are constant.\n",
            "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 96 members, which is less than n_splits=100.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 54.50% (+/- 23.55%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Non linear SVM - RFE\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "k = 100\n",
        "x_rfe = rfe_func(x_scaled,k)\n",
        "\n",
        "# Train non-linear SVM with cross-validation\n",
        "clf = SVC(kernel='poly',degree=3, C=1.0)\n",
        "scores = cross_val_score(clf, x_rfe, y_train, cv=100)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "PgWJfEmK0YEZ",
        "outputId": "836aeb33-c431-4e1b-a881-00ec83ba8772",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 96 members, which is less than n_splits=100.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.50% (+/- 29.58%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Non linear SVM - Lasso\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "k = 0.01\n",
        "x_lasso = lasso_func(x_scaled,k)\n",
        "\n",
        "# Train non-linear SVM with cross-validation\n",
        "clf = SVC(kernel='poly',degree=3, C=1.0)\n",
        "scores = cross_val_score(clf, x_lasso, y_train, cv=100)\n",
        "\n",
        "# Print cross-validation scores\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "5AhsUsRm1JX2",
        "outputId": "10ad2d68-b073-4185-aa5b-33f5122a373e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.106e+01, tolerance: 4.898e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 96 members, which is less than n_splits=100.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 52.00% (+/- 12.08%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForest ensemble poging - selectKBest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "k = 100\n",
        "x_kbest = kbest_func(x_scaled,k)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a random forest classifier object with n=10\n",
        "# clf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "# scores = cross_val_score(clf, x_kbest, y_train, cv=100)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "# print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "params = {'n_estimators': [1, 2, 5, 10, 20, 30, 40, 50, 100]}\n",
        "\n",
        "# Run grid search with 50 fold cross validation on forest classifier\n",
        "grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, cv=50)\n",
        "grid = grid.fit(x_kbest, y_train)\n",
        "\n",
        "print(grid.cv_results_)\n",
        "print(grid.best_estimator_)\n",
        "print(grid.best_score_)\n",
        "print(grid.best_params_)"
      ],
      "metadata": {
        "id": "qoVQZQ1b4GLS",
        "outputId": "5ce36adb-059f-4cb1-a0e8-305c264c6bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [ 77  83  84  90  97 103 110] are constant.\n",
            "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ]
        }
      ]
    }
  ]
}