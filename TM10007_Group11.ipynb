{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christievanderuit/TM10007_group11/blob/main/TM10007_Group11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn numpy matplotlib"
      ],
      "metadata": {
        "id": "cWwNaEqJ_iat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff2f3fe-554c-476e-d793-4e948ef65169"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2955 sha256=65bb7a5d4a64f6f2d7faa7c2a385de1e7e82b313ff73697d1c496c4f79d8b789\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/e0/3d/9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General packages\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection \n",
        "from sklearn import preprocessing\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "oS9eMkou8yR_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some functions we will use\n",
        "def colorplot(clf, ax, x, y, h=100):\n",
        "    '''\n",
        "    Overlay the decision areas as colors in an axes.\n",
        "    \n",
        "    Input:\n",
        "        clf: trained classifier\n",
        "        ax: axis to overlay color mesh on\n",
        "        x: feature on x-axis\n",
        "        y: feature on y-axis\n",
        "        h(optional): steps in the mesh\n",
        "    '''\n",
        "    # Create a meshgrid the size of the axis\n",
        "    xstep = (x.max() - x.min() ) / 20.0\n",
        "    ystep = (y.max() - y.min() ) / 20.0\n",
        "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
        "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
        "    h = max((x_max - x_min, y_max - y_min))/h\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    if hasattr(clf, \"decision_function\"):\n",
        "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    else:\n",
        "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    if len(Z.shape) > 1:\n",
        "        Z = Z[:, 1]\n",
        "    \n",
        "    # Put the result into a color plot\n",
        "    cm = plt.cm.RdBu_r\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm"
      ],
      "metadata": {
        "id": "Y8zNyBBH9BCh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "import os\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/tm10007_ml/worcgist/GIST_radiomicFeatures.csv', index_col=0)\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ],
      "metadata": {
        "id": "qX1LjCmltopI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ad9dea-0321-472d-f34c-0c136c2cd21e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tm10007_ml'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 83 (delta 13), reused 12 (delta 12), pack-reused 62\u001b[K\n",
            "Unpacking objects: 100% (83/83), 67.93 MiB | 7.34 MiB/s, done.\n",
            "The number of samples: 246\n",
            "The number of columns: 494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zorgen dat verschillende functies uit de files aangeroepen kunnen worden \n",
        "from tm10007_ml.worcgist.load_data import *   #misschien dat dit nog op een nettere manier kan \n",
        "dataset = load_data(); "
      ],
      "metadata": {
        "id": "bG7AZdOO5cDq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zoeken van NaN --> want duidelijk dat deze erin zitten - ook voor de nu geplotte in de eerste figuur \n",
        "x_data = data.drop(\"label\", axis='columns')\n",
        "\n",
        "\n",
        "np.where(np.asanyarray(np.isnan(x_data))) \n",
        "\n",
        "# # Find all entries that are not a number\n",
        "x_data = x_data.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
        "nan_b = pd.isna(x_train)\n",
        "n_nan = nan_b.sum().sum()\n",
        "if n_nan > 0:\n",
        "  nan = np.where(nan_b == \"True\")\n",
        "else:\n",
        "  nan = 0\n",
        "# # Replace all NaN with 0\n",
        "# x_train_nieuw = x_train.fillna(0)\n",
        "print(n_nan)\n",
        "\n",
        "# volgens mij hebben wij dus alleen rijen waarbij er opeens 0'en zijn, maar \n",
        "# hebben wij geen Nan \n",
        "#op deze manier worden in x_train alle kolommen gevonden waarbij er alleen maar \n",
        "#0'en zijn, dus deze droppen - dit zijn 6 kolommen \n",
        "df1 = x_data.mask(x_data != 0).dropna(axis=1)\n",
        "# print(df1)\n",
        "\n",
        "#zorgen dat deze rijen verwijderd worden ? willen we dit ?  NOG NIET GELUKT :)\n",
        "# column_names = list(df1.columns.values)\n",
        "# df2 = x_train.drop('index', inplace=True, axis=1)\n",
        "\n",
        "df1 = x_data.mask(x_data != 0).dropna(axis=1)\n",
        "\n",
        "df2 = x_data.copy()\n",
        "for col in df1:\n",
        "    df2 = df2.drop(col, axis=1)\n",
        "\n",
        "#print(df2)"
      ],
      "metadata": {
        "id": "X4w4SMYwvTtm",
        "outputId": "1fc2af9b-4737-4524-ed32-cf61db050353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitsen in een train en in een testset \n",
        "\n",
        "# Classifiers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Waarbij x = features, y = label\n",
        "#vraag: doet ie dit elke keer op een andere manier? want dan toch lastig conclusies te trekken? --> daarom achter random state een getal - maakt niet uit zolang integer is\n",
        "y = data['label']\n",
        "x = df2\n",
        "# df_train, df_test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1, stratify=y)\n",
        "#AFBLIJVEN VAN TESTDATA!! :) \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "akTrzKpG86zA"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testen of de data normaal verdeeld is of niet \n",
        "from scipy.stats import shapiro\n",
        "  \n",
        "# define a function to test for normality using the Shapiro-Wilk test\n",
        "def test_normality(column):\n",
        "    stat, p = shapiro(column)\n",
        "    alpha = 0.05\n",
        "    if p > alpha:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# apply the test_normality function to each column of the DataFrame\n",
        "normality_results = x_train.apply(test_normality)\n",
        "\n",
        "# print the results\n",
        "print(normality_results)\n",
        "# sum = sum(normality_results)\n",
        "print(sum)\n",
        "\n",
        "#dus meeste data is niet normaal verdeeld voor 60 van de gevallen --> dus voor deze wil je \n",
        "#een andere schaling gaan toepassen \n",
        "\n",
        "# losse dataframes maken \n",
        "normalized = pd.DataFrame()\n",
        "non_normalized = pd.DataFrame()\n",
        "for ind, elem in zip(x_train.columns, normality_results): \n",
        "  if elem == 0: \n",
        "    non_normalized = non_normalized.append(x_train[ind])\n",
        "  else: \n",
        "    normalized = normalized.append(x_train[ind])\n",
        "\n",
        "  \n",
        "\n",
        "#los scalen \n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(non_normalized)\n",
        "non_normalized = pd.DataFrame(scaler.transform(non_normalized))\n",
        "\n",
        "scaler_norm = preprocessing.StandardScaler()\n",
        "scaler_norm.fit(normalized)\n",
        "normalized = pd.DataFrame(scaler_norm.transform(normalized))\n",
        "\n",
        "#non_normalized en normalized gecombineerd in een nieuwe dataframe\n",
        "x_train_scaled = pd.concat([non_normalized, normalized])\n",
        "scaler_tot = preprocessing.MinMaxScaler()\n",
        "scaler_tot.fit(x_train_scaled)\n",
        "x_train_scaled = pd.DataFrame(scaler_tot.transform(x_train_scaled))\n",
        "\n",
        "#x_train_scaled = normalized.append(non_normalized)\n",
        "# 1. losse df voor normalized en non normalized \n",
        "# 2. scaling met Robust en Standard \n",
        "# 3. Samenvoegen en alles met Min max normalizeren \n"
      ],
      "metadata": {
        "id": "Lmhiy0kOuuh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1psx_DfGuq2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(non_normalized)"
      ],
      "metadata": {
        "id": "V1wizNPLJKMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotten \n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "print(type(x_train['PREDICT_original_hf_min']))\n",
        "plt.scatter(x_train['PREDICT_original_phasef_phasesym_peak_position_WL3_N5'], x_train['PREDICT_original_phasef_phasesym_range_WL3_N5'])\n",
        "# plt.show()\n",
        "\n",
        "#Testen met een andere variabele \n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "print(type(x_train['PREDICT_original_hf_min']))\n",
        "plt.scatter(x_train['PREDICT_original_sf_convexity_avg_2.5D'], x_train['PREDICT_original_phasef_phasesym_range_WL3_N5'])\n",
        "# plt.show()\n",
        "\n",
        "print(x_train)"
      ],
      "metadata": {
        "id": "DV4adj_J8qWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt at PCA\n",
        "\n",
        "def pca_func(X,n):\n",
        "  # create PCA object with 2 components\n",
        "  pca = PCA(n_components=n)\n",
        "\n",
        "  # fit and transform the data\n",
        "  df_pca = pca.fit_transform(X)\n",
        "  return df_pca\n",
        "\n",
        "# Hier kan je dus misschien een loopje van maken om te kijken wat het best werkt\n",
        "n = 50\n",
        "x_pca = pca_func(df2,n)"
      ],
      "metadata": {
        "id": "Ndqro_rJkWD8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SelectKBest \n",
        "\n",
        "def selectkbest_func(X,k):\n",
        "    # select the top n features using chi-squared test\n",
        "    select = SelectKBest(score_func=chi2, n_features=k)\n",
        "\n",
        "    # fit and transform the data\n",
        "    df_selectkbest = select.fit_transform(X)\n",
        "    return df_selectkbest"
      ],
      "metadata": {
        "id": "1btbKWNlLvc3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE\n",
        "\n",
        "def rfe_func(X,k):\n",
        "    # linear regression\n",
        "    lr = LinearRegression()\n",
        "\n",
        "    # Create an RFE object and select the top n features (we moeten misschien nog even letten op de step)\n",
        "    rfe = RFE(estimator=lr, n_features=k, step=1)\n",
        "  \n",
        "    #fit and transform the data\n",
        "    df_rfe = rfe.fit_transform(X)\n",
        "    return df_rfe"
      ],
      "metadata": {
        "id": "gIICUveAPhUC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L1 regularization (Lasso regression)\n",
        "\n",
        "def lasso_func(X,c): \n",
        "    #lasso regression\n",
        "    lasso = Lasso(alpha=c)\n",
        "\n",
        "    #fit and transform the data\n",
        "    df_lasso = lasso.fit_transform(X)\n",
        "    return df_lasso"
      ],
      "metadata": {
        "id": "UFsohvQ1URR2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with cross vallidation\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "y_gist = pd.Series([1 if label == \"GIST\" else 0 for label in y_train], index=y_train.index)\n",
        "\n",
        "\n",
        "# Create a linear SVM object\n",
        "svm = LinearSVC(max_iter=1000)\n",
        "\n",
        "# Perform 5-fold cross-validation on the training data\n",
        "scores = cross_val_score(svm, x_pca, y_gist, cv=50)\n",
        "\n",
        "# Print the average accuracy and standard deviation of the cross-validation scores\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))\n"
      ],
      "metadata": {
        "id": "G7QNF2o9rY_2",
        "outputId": "1295abb9-9e8f-4bf9-b754-b2716593753b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 51.17% (+/- 7.64%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "scores = cross_val_score(knn, x_pca, y_gist, cv=100)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))\n"
      ],
      "metadata": {
        "id": "jjfT4toUuRbu",
        "outputId": "cc73c4d2-cabb-465e-add2-fd6bcfb1b35f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 55.50% (+/- 31.54%)\n"
          ]
        }
      ]
    }
  ]
}