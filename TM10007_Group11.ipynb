{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christievanderuit/TM10007_group11/blob/Caitlyn/TM10007_Group11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn numpy matplotlib"
      ],
      "metadata": {
        "id": "cWwNaEqJ_iat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb211d1f-3eab-4abe-9f28-22b13d7d3db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2955 sha256=cd1602b6d3eaeec227351044d6f45940c230dcbf6424b37e97f5894baa36f0f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/e0/3d/9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General packages\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection \n",
        "from sklearn import preprocessing\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "\n"
      ],
      "metadata": {
        "id": "oS9eMkou8yR_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some functions we will use\n",
        "def colorplot(clf, ax, x, y, h=100):\n",
        "    '''\n",
        "    Overlay the decision areas as colors in an axes.\n",
        "    \n",
        "    Input:\n",
        "        clf: trained classifier\n",
        "        ax: axis to overlay color mesh on\n",
        "        x: feature on x-axis\n",
        "        y: feature on y-axis\n",
        "        h(optional): steps in the mesh\n",
        "    '''\n",
        "    # Create a meshgrid the size of the axis\n",
        "    xstep = (x.max() - x.min() ) / 20.0\n",
        "    ystep = (y.max() - y.min() ) / 20.0\n",
        "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
        "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
        "    h = max((x_max - x_min, y_max - y_min))/h\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    if hasattr(clf, \"decision_function\"):\n",
        "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    else:\n",
        "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    if len(Z.shape) > 1:\n",
        "        Z = Z[:, 1]\n",
        "    \n",
        "    # Put the result into a color plot\n",
        "    cm = plt.cm.RdBu_r\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm"
      ],
      "metadata": {
        "id": "Y8zNyBBH9BCh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "import os\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/tm10007_ml/worcgist/GIST_radiomicFeatures.csv', index_col=0)\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ],
      "metadata": {
        "id": "qX1LjCmltopI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672df4ae-4561-42cb-a6bc-380beac4b12c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n",
            "The number of samples: 246\n",
            "The number of columns: 494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zorgen dat verschillende functies uit de files aangeroepen kunnen worden \n",
        "from tm10007_ml.worcgist.load_data import *   #misschien dat dit nog op een nettere manier kan \n",
        "dataset = load_data(); "
      ],
      "metadata": {
        "id": "bG7AZdOO5cDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitsen in een train en in een testset \n",
        "\n",
        "# Classifiers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Waarbij x = features, y = label\n",
        "#vraag: doet ie dit elke keer op een andere manier? want dan toch lastig conclusies te trekken? --> daarom achter random state een getal - maakt niet uit zolang integer is\n",
        "y = data['label']\n",
        "x = data.drop(\"label\", axis='columns')\n",
        "# df_train, df_test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1, stratify=y)\n",
        "#AFBLIJVEN VAN TESTDATA!! :) \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "akTrzKpG86zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testen of de data normaal verdeeld is of niet \n",
        "from scipy.stats import shapiro\n",
        "  \n",
        "# define a function to test for normality using the Shapiro-Wilk test\n",
        "def test_normality(column):\n",
        "    stat, p = shapiro(column)\n",
        "    alpha = 0.05\n",
        "    if p > alpha:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# apply the test_normality function to each column of the DataFrame\n",
        "normality_results = x_train.apply(test_normality)\n",
        "\n",
        "# print the results\n",
        "print(normality_results)\n",
        "# sum = sum(normality_results)\n",
        "print(sum)\n",
        "\n",
        "#dus meeste data is niet normaal verdeeld voor 60 van de gevallen --> dus voor deze wil je \n",
        "#een andere schaling gaan toepassen \n",
        "\n",
        "# losse dataframes maken \n",
        "normalized = pd.DataFrame()\n",
        "non_normalized = pd.DataFrame()\n",
        "for ind, elem in zip(x_train.columns, normality_results): \n",
        "  if elem == 0: \n",
        "    non_normalized = non_normalized.append(x_train[ind])\n",
        "  else: \n",
        "    normalized = normalized.append(x_train[ind])\n",
        "  \n",
        "\n",
        "#los scalen \n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(non_normalized)\n",
        "non_normalized = pd.DataFrame(scaler.transform(non_normalized))\n",
        "\n",
        "scaler_norm = preprocessing.StandardScaler()\n",
        "scaler_norm.fit(normalized)\n",
        "normalized = pd.DataFrame(scaler_norm.transform(normalized))\n",
        "\n",
        "#non_normalized en normalized gecombineerd in een nieuwe dataframe\n",
        "x_train_scaled = pd.concat([non_normalized, normalized])\n",
        "scaler_tot = preprocessing.MinMaxScaler()\n",
        "scaler_tot.fit(x_train_scaled)\n",
        "x_train_scaled = pd.DataFrame(scaler_tot.transform(x_train_scaled))\n",
        "\n",
        "#x_train_scaled = normalized.append(non_normalized)\n",
        "# 1. losse df voor normalized en non normalized \n",
        "# 2. scaling met Robust en Standard \n",
        "# 3. Samenvoegen en alles met Min max normalizeren \n"
      ],
      "metadata": {
        "id": "Lmhiy0kOuuh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(non_normalized)"
      ],
      "metadata": {
        "id": "V1wizNPLJKMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotten \n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "print(type(x_train['PREDICT_original_hf_min']))\n",
        "plt.scatter(x_train['PREDICT_original_phasef_phasesym_peak_position_WL3_N5'], x_train['PREDICT_original_phasef_phasesym_range_WL3_N5'])\n",
        "# plt.show()\n",
        "\n",
        "#Testen met een andere variabele \n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "print(type(x_train['PREDICT_original_hf_min']))\n",
        "plt.scatter(x_train['PREDICT_original_sf_convexity_avg_2.5D'], x_train['PREDICT_original_phasef_phasesym_range_WL3_N5'])\n",
        "# plt.show()\n",
        "\n",
        "print(x_train)"
      ],
      "metadata": {
        "id": "DV4adj_J8qWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zoeken van NaN --> want duidelijk dat deze erin zitten - ook voor de nu geplotte in de eerste figuur \n",
        "\n",
        "\n",
        "np.where(np.asanyarray(np.isnan(x_train))) \n",
        "\n",
        "# # Find all entries that are not a number\n",
        "x_train = x_train.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
        "nan_b = pd.isna(x_train)\n",
        "n_nan = nan_b.sum().sum()\n",
        "if n_nan > 0:\n",
        "  nan = np.where(nan_b == \"True\")\n",
        "else:\n",
        "  nan = 0\n",
        "# # Replace all NaN with 0\n",
        "# x_train_nieuw = x_train.fillna(0)\n",
        "print(n_nan)\n",
        "\n",
        "# volgens mij hebben wij dus alleen rijen waarbij er opeens 0'en zijn, maar \n",
        "# hebben wij geen Nan \n",
        "#op deze manier worden in x_train alle kolommen gevonden waarbij er alleen maar \n",
        "#0'en zijn, dus deze droppen - dit zijn 6 kolommen \n",
        "df1 = x_train.mask(x_train != 0).dropna(axis=1)\n",
        "# print(df1)\n",
        "\n",
        "#zorgen dat deze rijen verwijderd worden ? willen we dit ?  NOG NIET GELUKT :)\n",
        "# column_names = list(df1.columns.values)\n",
        "# df2 = x_train.drop('index', inplace=True, axis=1)\n",
        "\n",
        "df1 = x_train.mask(x_train != 0).dropna(axis=1)\n",
        "\n",
        "df2 = x_train.copy()\n",
        "for col in df1:\n",
        "    df2 = df2.drop(col, axis=1)\n"
      ],
      "metadata": {
        "id": "eAztzTpyDUwP",
        "outputId": "fd49731e-3b62-4ca9-bcbb-67f45ee8f9a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt at PCA\n",
        "\n",
        "def pca_func(X,n):\n",
        "  # create PCA object with 2 components\n",
        "  pca = PCA(n_components=n)\n",
        "\n",
        "  # fit and transform the data\n",
        "  df_pca = pca.fit_transform(X)\n",
        "  return df_pca\n",
        "\n",
        "# Hier kan je dus misschien een loopje van maken om te kijken wat het best werkt\n",
        "n = 100\n",
        "df_pca_test = pca_func(df2,n)"
      ],
      "metadata": {
        "id": "Ndqro_rJkWD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SelectKBest \n",
        "\n",
        "def selectkbest_func(X,k):\n",
        "    # select the top n features using chi-squared test\n",
        "    select = SelectKBest(score_func=chi2, n_features=k)\n",
        "\n",
        "    # fit and transform the data\n",
        "    df_selectkbest = select.fit_transform(X)\n",
        "    return df_selectkbest"
      ],
      "metadata": {
        "id": "1btbKWNlLvc3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RFE\n",
        "\n",
        "def rfe_func(X,k):\n",
        "    # linear regression\n",
        "    lr = LinearRegression()\n",
        "\n",
        "    # Create an RFE object and select the top n features (we moeten misschien nog even letten op de step)\n",
        "    rfe = RFE(estimator=lr, n_features=k, step=1)\n",
        "  \n",
        "    #fit and transform the data\n",
        "    df_rfe = rfe.fit_transform(X)\n",
        "    return df_rfe"
      ],
      "metadata": {
        "id": "gIICUveAPhUC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L1 regularization (Lasso regression)\n",
        "\n",
        "def lasso_func(X,c): \n",
        "    #lasso regression\n",
        "    lasso = Lasso(alpha=c)\n",
        "\n",
        "    #fit and transform the data\n",
        "    df_lasso = lasso.fit_transform(X)\n",
        "    return df_lasso"
      ],
      "metadata": {
        "id": "UFsohvQ1URR2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparing different feature selection methods\n",
        "\n",
        "methods = [(\"f_classif_vs_SelectKBest\", SelectKBest(f_classif)),\n",
        "           (\"mutual_info_vs_SelectKBest\", SelectKBest(mutual_info_classif)),\n",
        "           (\"lasso_vs_SelectKBest\", SelectKBest(Lasso)),\n",
        "           (\"f_classif_vs_lasso\", df_lasso(f_classif)),\n",
        "           (\"mutual_info_vs_lasso\", Lasso(mutual_info_classif)),]\n",
        "\n",
        "# (\"f_classif_vs_mutual_info\", mutual_info_classif(f_classif)),\n",
        "\n",
        "# Loop over the feature selection methods\n",
        "for name, method in methods:\n",
        "    \n",
        "    # Fit the feature selection method on the training data\n",
        "    X_train_new = method.fit_transform(x_train, y_train)\n",
        "    \n",
        "    # Get the indices of the selected features\n",
        "    selected_features = method.get_support(indices=True)\n",
        "    \n",
        "    # Print the selected features\n",
        "    print(\"Selected features using\", name, \":\", x.columns[selected_features])\n",
        "    \n",
        "    # Define the classifiers to compare\n",
        "    classifiers = [(\"Random Forest\", RandomForestClassifier(random_state=42)),\n",
        "                   (\"Gradient Boosting\", GradientBoostingClassifier(random_state=42)),\n",
        "                   (\"SVM\", SVC(random_state=42)),\n",
        "                   (\"KNN\", KNeighborsClassifier())]\n",
        "    \n",
        "    # Loop over the classifiers\n",
        "    for clf_name, clf in classifiers:\n",
        "        \n",
        "        # Train the classifier on the selected features and evaluate its performance\n",
        "        clf.fit(X_train_new, y_train)\n",
        "        cv_score = cross_val_score(clf, X_train_new, y_train, cv=5)\n",
        "        print(\"Cross-validation accuracy using\", clf_name, \":\", np.mean(cv_score))\n",
        "        \n",
        "        # Transform the test data using the same feature selection method\n",
        "        #X_test_new = method.transform(X_test)\n",
        "        \n",
        "        # Evaluate the performance of the classifier on the test data\n",
        "        #test_score = clf.score(X_test_new, y_test)\n",
        "        #print(\"Test accuracy using\", clf_name, \":\", test_score)\n"
      ],
      "metadata": {
        "id": "QP4zSRygWOQE",
        "outputId": "6a6c80c6-a059-4892-fc6f-b9da4f2beaa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-5cdb653ef62a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0;34m\"mutual_info_vs_SelectKBest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmutual_info_classif\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0;34m\"lasso_vs_SelectKBest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLasso\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m            \u001b[0;34m(\u001b[0m\u001b[0;34m\"f_classif_vs_lasso\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_lasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m            (\"mutual_info_vs_lasso\", Lasso(mutual_info_classif)),]\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_lasso' is not defined"
          ]
        }
      ]
    }
  ]
}