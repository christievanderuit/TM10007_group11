{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christievanderuit/TM10007_group11/blob/main/TM10007_Group11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn numpy matplotlib"
      ],
      "metadata": {
        "id": "cWwNaEqJ_iat",
        "outputId": "ff6cce4a-035c-40fd-85bf-41f335a5daf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.9/dist-packages (0.0.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General packages\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection \n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import neighbors\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oS9eMkou8yR_"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to use from colab environment\n",
        "!git clone https://github.com/jveenland/tm10007_ml.git\n",
        "data = pd.read_csv('/content/tm10007_ml/worcgist/GIST_radiomicFeatures.csv', index_col=0)\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ],
      "metadata": {
        "id": "qX1LjCmltopI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7594a916-2704-4abe-ab84-d808055c45bf"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n",
            "The number of samples: 246\n",
            "The number of columns: 494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zoeken van NaN --> want duidelijk dat deze erin zitten - ook voor de nu geplotte in de eerste figuur \n",
        "x_data = data.drop(\"label\", axis='columns')\n",
        "\n",
        "\n",
        "np.where(np.asanyarray(np.isnan(x_data))) \n",
        "\n",
        "# Find all entries that are not a number\n",
        "x_data = x_data.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
        "nan_b = pd.isna(x_data)\n",
        "n_nan = nan_b.sum().sum()\n",
        "if n_nan > 0:\n",
        "  nan = np.where(nan_b == \"True\")\n",
        "else:\n",
        "  nan = 0\n",
        "\n",
        "#0'en zijn, dus deze droppen - dit zijn 6 kolommen \n",
        "df1 = x_data.mask(x_data != 0).dropna(axis=1)\n",
        "\n",
        "df2 = x_data.copy()\n",
        "for col in df1:\n",
        "    df2 = df2.drop(col, axis=1)\n"
      ],
      "metadata": {
        "id": "X4w4SMYwvTtm"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitsen in een train en in een testset \n",
        "\n",
        "#Waarbij x = features, y = label\n",
        "y = data['label']\n",
        "x = df2\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=1, stratify=y)\n",
        "#AFBLIJVEN VAN TESTDATA!!! :) \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "akTrzKpG86zA"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testen of er outliers zijn dmv LOF\n",
        "\n",
        "lof = LocalOutlierFactor()\n",
        "x_lof = lof.fit(x_train)\n",
        "outlier_scores = x_lof.negative_outlier_factor_\n",
        "threshold = np.mean(outlier_scores)\n",
        "outliers = x_train[outlier_scores < threshold]\n",
        "outlier_percentage = (len(outliers) / len(x_train)) * 100\n",
        "\n",
        "print(\"Number of outliers:\", len(outliers))\n",
        "print(\"Percentage of outliers:\", outlier_percentage)"
      ],
      "metadata": {
        "id": "sR_vwqT-JiEi",
        "outputId": "c9e280b2-b9e3-4a13-a2e4-70a14b4fbd6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of outliers: 9\n",
            "Percentage of outliers: 4.591836734693878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test whether the data is normally distributed or not\n",
        "\n",
        "# define a function to test for normality using the Shapiro-Wilk test\n",
        "def test_normality(column):\n",
        "    stat, p = shapiro(column)\n",
        "    alpha = 0.05\n",
        "    if p > alpha:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# apply the test_normality function to each column of the DataFrame\n",
        "normality_results = x_train.apply(test_normality)\n",
        "\n",
        "# combine the normalized and non-normalized data\n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(x_train)\n",
        "x_scaled = pd.DataFrame(scaler.transform(x_train))"
      ],
      "metadata": {
        "id": "SUq8vAe8isP4"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for different feature extraction/selection methods"
      ],
      "metadata": {
        "id": "MJsCQ9aC07MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "\n",
        "def pca_func(X,n):\n",
        "  # create PCA object with 2 components\n",
        "  pca = PCA(n_components=n)\n",
        "\n",
        "  # fit and transform the data\n",
        "  df_pca = pca.fit_transform(X)\n",
        "  return df_pca"
      ],
      "metadata": {
        "id": "Ndqro_rJkWD8"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def kbest_func(X, k):\n",
        "    \"\"\"\n",
        "    Select the top k features using the f_regression score as the evaluation metric\n",
        "    \"\"\"\n",
        "    # define the feature selector\n",
        "    selector = SelectKBest(f_classif, k=k)\n",
        "    \n",
        "    # fit the selector to the training data and transform the data\n",
        "    x_train_kbest = selector.fit_transform(X, y_train)\n",
        "    return x_train_kbest"
      ],
      "metadata": {
        "id": "1btbKWNlLvc3"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rfe_func(X,k): \n",
        "    # Create a LabelEncoder object \n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Fit the LabelEncoder object on the target variable (dit wordt gedaan omdat y_train uit alleen maar strings bestaat en dan kan je niet RFE gebruiken)\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "\n",
        "    # Create a logistic regression object\n",
        "    lr = LinearRegression()\n",
        "\n",
        "    # Create an RFE object and fit it on the training data\n",
        "    rfe = RFE(estimator=lr, n_features_to_select=k)\n",
        "    x_train_rfe = rfe.fit_transform(X, y_train_encoded)\n",
        "    return x_train_rfe"
      ],
      "metadata": {
        "id": "gIICUveAPhUC"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L1 regularization (Lasso regression)\n",
        "\n",
        "def lasso_func(X,k):   \n",
        "    # Create a LabelEncoder object\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # Fit the LabelEncoder object on the target variable\n",
        "    y_train_encoded = le.fit_transform(y_train)\n",
        "\n",
        "    # Lasso regression\n",
        "    lasso = Lasso(alpha=k)\n",
        "    \n",
        "    # Create a SelectFromModel object and fit it on the training data\n",
        "    selector = SelectFromModel(estimator=lasso)\n",
        "    selector.fit(X, y_train_encoded)\n",
        "\n",
        "    # Use the selector object to transform the data\n",
        "    x_train_lasso = selector.transform(X)\n",
        "    return x_train_lasso"
      ],
      "metadata": {
        "id": "UFsohvQ1URR2"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick k for KNN without feature extraction/selection\n",
        "y_gist = pd.Series([1 if label == \"GIST\" else 0 for label in y_train], index=y_train.index)\n",
        "\n",
        "# Define a list of k values to test\n",
        "k_values = list(range(1, 50, 2))\n",
        "\n",
        "# Train a KNN classifier for each k value and record the test set accuracy\n",
        "errors = []\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_val_score(knn, x_scaled, y_gist, cv=5)\n",
        "    errors.append(1-scores.mean())\n",
        "\n",
        "# Create a line plot of k vs. test set accuracy\n",
        "plt.plot(k_values, errors)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Test Set Error')\n",
        "plt.title('KNN Classifier Performance')\n",
        "plt.show()\n",
        "print(k_values)"
      ],
      "metadata": {
        "id": "k6Oo6NoDk9_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing which feature selection method works best using KNN. Hyperparameters: CV = 5, n_neighbours = 15, n/k = [5 10 20 50 100]."
      ],
      "metadata": {
        "id": "1xl1ErUOkcJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging - PCA\n",
        "n = 100\n",
        "x_pca = pca_func(x_scaled,n)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "scores = cross_val_score(knn, x_pca, y_gist, cv=5)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))\n"
      ],
      "metadata": {
        "id": "jjfT4toUuRbu",
        "outputId": "4ade773b-4c08-4e94-d7e7-46266d0c055d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 56.13% (+/- 3.29%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging - selectKBest\n",
        "\n",
        "k = 50\n",
        "x_kbest = kbest_func(x_scaled,k)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "knn_select_scores = cross_val_score(knn, x_kbest, y_gist, cv=5)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(knn_select_scores.mean()*100, knn_select_scores.std()*100))"
      ],
      "metadata": {
        "id": "aqx1AjZsTW3d",
        "outputId": "ae70320a-29bd-48bf-8b40-1e598b904ba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.22% (+/- 4.31%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging - RFE\n",
        "\n",
        "k = 100\n",
        "x_rfe = rfe_func(x_scaled,k)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "scores = cross_val_score(knn, x_rfe, y_gist, cv=5)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "5VXKQLHwo0Sc",
        "outputId": "44d08617-4bb7-4260-c592-85d5c7693321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 59.72% (+/- 4.44%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN poging - lasso\n",
        "\n",
        "k = 10\n",
        "x_lasso = lasso_func(x_scaled, k)\n",
        "\n",
        "# Assume X_train, y_train are numpy arrays or pandas dataframes\n",
        "# Create a k-NN classifier object with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "# Use 5-fold cross-validation to evaluate the performance of the k-NN classifier on the training set\n",
        "scores = cross_val_score(knn, x_lasso, y_gist, cv=5)\n",
        "\n",
        "# Print the cross-validation scores and their mean\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))"
      ],
      "metadata": {
        "id": "UurZYnzVq89W",
        "outputId": "8a2e15de-63ab-4e1e-e38c-07e2bb699b01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 53.08% (+/- 4.26%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results of the best feature selection/extraction combined with KNN, Non-linear SVM and Random Forrest. Also hyperparameter optimilization."
      ],
      "metadata": {
        "id": "DxoZzJwSnGv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForest ensemble with cross_val - selectKBest\n",
        "\n",
        "k = 100\n",
        "x_kbest = kbest_func(x_scaled,k)\n",
        "\n",
        "params = {'n_estimators': [1, 2, 5, 10, 20, 30, 40, 50, 100]}\n",
        "\n",
        "# Run grid search with 50 fold cross validation on forest classifier\n",
        "grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, cv=5)\n",
        "grid = grid.fit(x_kbest, y_train)\n",
        "\n",
        "print(grid.cv_results_)\n",
        "print(grid.best_estimator_)\n",
        "print(grid.best_score_)\n",
        "print(grid.best_params_)"
      ],
      "metadata": {
        "id": "qoVQZQ1b4GLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forrest attempt Harmen ROC outcome\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "# Create a random forest classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Fit the random forest classifier on the training data\n",
        "rfc.fit(x_scaled, y_train)\n",
        "\n",
        "# Evaluate the random forest classifier on the testing data\n",
        "scores = cross_val_score(rfc, x_scaled, y_train, cv=10)\n",
        "scores_roc = cross_val_score(rfc, x_scaled, y_train, cv=10, scoring='roc_auc')\n",
        "\n",
        "\n",
        "print(\"Accuracy: {:.2f}% (+/- {:.2f}%)\".format(scores.mean()*100, scores.std()*100))\n",
        "print(\"ROC: {:.2f}% (+/- {:.2f}%)\".format(scores_roc.mean()*100, scores_roc.std()*100))"
      ],
      "metadata": {
        "id": "9321wESSIajE",
        "outputId": "61f490ff-1ecf-4fcb-fb41-897ac9635c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 61.74% (+/- 7.98%)\n",
            "ROC: 64.61% (+/- 9.79%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forrest with grid search Harmen\n",
        "\n",
        "# Define the hyperparameter grid to search over\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15, 20, 50, None],\n",
        "    'min_samples_split': [2, 5, 10, 20, 50]\n",
        "}\n",
        "\n",
        "# Create a grid search object with the RandomForestClassifier and hyperparameter grid\n",
        "grid_search = GridSearchCV(RandomForestClassifier(n_estimators=100), param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the grid search object to the training data\n",
        "grid_search.fit(x_scaled, y_train)\n",
        "\n",
        "# Print the best hyperparameters and the corresponding score on the validation set\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Validation set score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "uiQhaDjsu3Hi",
        "outputId": "e1af0f31-6f65-49a7-e8df-26a24f7a59d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'max_depth': None, 'min_samples_split': 10}\n",
            "Validation set score: 0.6482051282051282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC first attempt Christie\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "# gebruik maken van de KNN poging met selectKbest\n",
        "RocCurveDisplay.from_estimator(knn_select_scores, x_kbest, y_train)\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "jwUqw5VNw20S",
        "outputId": "b2de41c8-9067-4909-d46b-ca4c7f191be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-4cb1863e9896>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# gebruik maken van de KNN poging met selectKbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mRocCurveDisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_select_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_kbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'knn_select_scores' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC second attempt Christie\n",
        "from sklearn.metrics import roc_curve\n",
        "# Assuming you have already trained and predicted using your classification model\n",
        "y_pred = knn_select_scores.predict_proba(x_kbest)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KHJzUtHT5AVW",
        "outputId": "2934c946-ce68-49ea-c46b-5c77e0107d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-3ed956faaf2c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming you have already trained and predicted using your classification model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_select_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_kbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'knn_select_scores' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Evaluatie van het model uiteindelijk met outer cross validatie \n",
        "# from sklearn.model_selection import KFold \n",
        "# from sklearn.metrics import accuracy_score \n",
        "\n",
        "# #dit moet helemaal bovenaan de code \n",
        "# from sklearn.model_selection import train_test_split, KFold, GridSearchCV \n",
        "\n",
        "# y = data['label']\n",
        "# x = df2\n",
        "# # Define outer cross-validation\n",
        "# outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# outer_scores = []\n",
        "\n",
        "# for train_index, test_index in outer_cv.split(x):\n",
        "#     # Split data into outer training and test sets\n",
        "#     X_train_outer, X_test = x[train_index], x[test_index] # dit zorgt ervoor dat hij hem splitst in outer in een train en test set \n",
        "#     y_train_outer, y_test = y[train_index], y[test_index]\n",
        "\n",
        "#     # Define inner cross-validation\n",
        "#     inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "#     inner_scores = []\n",
        "\n",
        "#     for inner_train_index, inner_test_index in inner_cv.split(X_train_outer):\n",
        "#         # Split data into inner training and validation sets      --> hier moet de code komen die we al hebben met de juiste classifier --> kunnen het makkelijk maken door hier gewoon de nieuwe classifier in te plakkken, wel beetje omslachtig maar wel t makkelijkste \n",
        "#         X_train_inner, X_val = X_train_outer[inner_train_index], X_train_outer[inner_test_index]\n",
        "#         y_train_inner, y_val = y_train_outer[inner_train_index], y_train_outer[inner_test_index]\n",
        "\n",
        "#         # Split inner training set into training and validation sets\n",
        "#         X_train, X_test_inner, y_train, y_test_inner = train_test_split(X_train_inner, y_train_inner, test_size=0.2, random_state=42)\n",
        "\n",
        "# # hier komt ons model + scaling en feature selectie \n",
        "       \n",
        "#        #LET OP HIER MOET DE JUISTE SCALING WORDEN AANGEPAST \n",
        "#         # scale data\n",
        "#         scaler = preprocessing.RobustScaler()\n",
        "#         scaler.fit(x_train)\n",
        "#         x_scaled = pd.DataFrame(scaler.transform(non_normalized))\n",
        "\n",
        "\n",
        "#       #HIER MOET DE CODE KOMEN DIE UITEINDELIJK GEKOZEN IS VOOR DE CLASSIFIER \n",
        "#         # Evaluate model on inner validation set\n",
        "#         inner_score = grid_search.score(X_val, y_val)\n",
        "#         inner_scores.append(inner_score)\n",
        "\n",
        "#     # Train model on outer training set with best hyperparameters\n",
        "#     best_params = grid_search.best_params_\n",
        "#     model = SVC(**best_params)\n",
        "#     model.fit(X_train_outer, y_train_outer)\n",
        "\n",
        "#     # Evaluate model on outer test set\n",
        "#     outer_score = model.score(X_test, y_test)\n",
        "#     outer_scores.append(outer_score)\n",
        "\n",
        "# # Print average outer cross-validation score\n",
        "# print('Outer CV score:', np.mean(outer_scores))"
      ],
      "metadata": {
        "id": "JDWNzzpo4qXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    axes : array of 3 axes, optional (default=None)\n",
        "        Axes to use for plotting the curves.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 5-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - :term:`CV splitter`,\n",
        "          - An iterable yielding (train, test) splits as arrays of indices.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : int or None, optional (default=None)\n",
        "        Number of jobs to run in parallel.\n",
        "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
        "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
        "        for more details.\n",
        "\n",
        "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
        "        Relative or absolute numbers of training examples that will be used to\n",
        "        generate the learning curve. If the dtype is float, it is regarded as a\n",
        "        fraction of the maximum size of the training set (that is determined\n",
        "        by the selected validation method), i.e. it has to be within (0, 1].\n",
        "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
        "        Note that for classification the number of samples usually have to\n",
        "        be big enough to contain at least one sample from each class.\n",
        "        (default: np.linspace(0.1, 1.0, 5))\n",
        "    \"\"\"\n",
        "\n",
        "    axes.set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes.set_ylim(*ylim)\n",
        "    axes.set_xlabel(\"Training examples\")\n",
        "    axes.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores  = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes.grid()\n",
        "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes.legend(loc=\"best\")\n",
        "\n",
        "    return plt\n",
        "\n",
        "#knn gebruiken als een classifier \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=9)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1) \n",
        "clf = knn\n",
        "title = \"knn\"\n",
        "plot_learning_curve(clf, title, x, y, ax, ylim=(0.3, 1.01), cv=10)\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1) \n",
        "title = \"descision tree\"\n",
        "plot_learning_curve(clf, title, x, y, ax, ylim=(0.3, 1.01), cv=10)\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(1, 1, 1) \n",
        "title = \"Gaussian NB\"\n",
        "plot_learning_curve(clf, title, x, y, ax, ylim=(0.3, 1.01), cv=10)\n"
      ],
      "metadata": {
        "id": "sXiHbITgiFu3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}